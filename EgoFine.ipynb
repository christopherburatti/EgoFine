{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251590e0-6fb8-483a-bd90-d38b6660220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd1630-ef7d-40c4-85df-b3ea77dad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1ffb-2ad8-40a5-8619-435f53de64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "newpath = 'experiments/EXP' + EXP\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "if not os.path.exists(newpath + 'model'):\n",
    "    os.makedirs(newpath + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf41ff-56e0-4d75-980b-1889167b22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b50ed-01bb-4c32-9de4-e2fc48614962",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d47866-1e41-44a9-ae55-ec2a0673b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph = nx.read_graphml(\"GRAPH_PATH.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21c907-953b-4943-8a7c-a8e2f17f1b49",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534c9c6-b68e-4f91-8084-f8980180a08a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_triples = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110fca7-7540-4112-a43f-c14efa750abe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "r = random.randint(29,49)\n",
    "while len(test_triples) < r:\n",
    "    nodes = random.sample(graph.nodes(), k=2)\n",
    "    if not graph.has_edge(nodes[0], nodes[1]) and not graph.has_edge(nodes[1], nodes[0]):\n",
    "        t = (nodes[0], 'no relation', nodes[1])\n",
    "        test_triples.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce7b8a-5aa2-4d29-9861-ecd3dca20089",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9096a-8d9f-4fb8-9543-942342d6b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = sorted(graph.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d36da-8a45-4141-bdff-b5e8ebc7f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_remove = set()\n",
    "cnt = 0\n",
    "\n",
    "while len(test_triples) < 500:\n",
    "    edge = random.sample(edges, k=1)[0]\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    triple = (node1, edge[2]['relation'], node2)\n",
    "    test_triples.add(triple)\n",
    "    t = (edge[0], edge[1])\n",
    "    if graph.has_edge(edge[0], edge[1]):\n",
    "        graph.remove_edge(edge[0], edge[1])\n",
    "        cnt += 1\n",
    "    edges_to_remove.add(t)\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e6912-62f5-42de-b85b-f67b6270b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b50c7-4cb8-4fc0-b917-03675019bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/test_triples.pkl', 'wb') as file1:\n",
    "    pickle.dump(test_triples, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046ba8f-91c4-434d-abd3-3f04eaaef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/edges_to_remove.pkl', 'wb') as file1:\n",
    "    pickle.dump(edges_to_remove, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb9a24-8845-4216-8c19-76892fb3bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "ground_truth = []\n",
    "\n",
    "for t in test_triples:\n",
    "    chat = []\n",
    "    system = {\"role\": \"system\", \"content\": \"You are a chatbot that has to predict the relationship between nodes.\"}\n",
    "    user = {\"role\": \"user\", \"content\": \"Which is the relationship between the node '\" + t[0] + \"' and the node '\" + t[2] + \"'?\"}\n",
    "    chat.append(system)\n",
    "    chat.append(user)\n",
    "    test_dataset.append(chat)\n",
    "\n",
    "    assistant = {\"role\": \"assistant\", \"content\": t[1]}\n",
    "    chat.append(assistant)\n",
    "    ground_truth.append(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cddc7e-68af-4661-bd91-ddda124de4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/test_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(test_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da78f4-71f6-4e03-ae1c-1f1606bfec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/ground_truth.pkl', 'wb') as file1:\n",
    "    pickle.dump(ground_truth, file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5456ee-38de-4af0-9164-62f1182a1ffd",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f64e69-1bd2-4920-a022-9e66c9ecbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/edges_to_remove.pkl', 'rb') as file:\n",
    "    edges_to_remove = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cb1fa-d9af-4a64-bf38-c1b351835bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for e in edges_to_remove:\n",
    "    if graph.has_edge(e[0], e[1]):\n",
    "        graph.remove_edge(e[0], e[1])\n",
    "        cnt += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c0bb4-4baf-4673-96a4-ad9e60dc2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba02bf-6b76-4dd2-a0e1-78e30e39bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = list(nx.degree_centrality(graph).items())\n",
    "deg = sorted(degree_centrality, key=lambda x: x[1], reverse=True)\n",
    "del degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49da9cb-09a4-4d43-84b5-982fd3fbcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c0c09-27ac-4661-a1b7-8187aded375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "m_n_values = []\n",
    "ego_nodes_tot = []\n",
    "\n",
    "RADIUS_EGO = 2\n",
    "\n",
    "num_chat = 0\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "ego1 = nx.ego_graph(graph, deg[0][0], radius=RADIUS_EGO)\n",
    "m = int(nx.diameter(ego1))\n",
    "n = int(ego1.number_of_nodes()/m)\n",
    "\n",
    "random_path = nx.generate_random_paths(ego1, n, path_length=m)#, seed=42)\n",
    "paths = []\n",
    "for rp in random_path:\n",
    "    paths.append(rp)\n",
    "    num_chat += 1\n",
    "l = len(paths)\n",
    "if l > 0:\n",
    "    train_set.append(paths)\n",
    "    m_n = (m, n)\n",
    "    m_n_values.append(m_n)\n",
    "ego_nodes_tot.append(deg[0][0])\n",
    "\n",
    "ego1_nodes = list(ego1.nodes())\n",
    "del ego1\n",
    "\n",
    "for node, value in deg[1:]:\n",
    "    ego2 = nx.ego_graph(graph, node, radius=RADIUS_EGO)\n",
    "    ego2_nodes = list(ego2.nodes())\n",
    "    if jaccard_similarity(ego1_nodes, ego2_nodes) < 0.1:\n",
    "        m = int(nx.diameter(ego2))\n",
    "        n = int(ego2.number_of_nodes()/m)\n",
    "        random_path = nx.generate_random_paths(ego2, n, path_length=m)#, seed=42)\n",
    "        paths = []\n",
    "        for rp in random_path:\n",
    "            paths.append(rp)\n",
    "            num_chat += 1\n",
    "\n",
    "        train_set.append(paths)\n",
    "        m_n = (m, n)\n",
    "        m_n_values.append(m_n)\n",
    "        ego_nodes_tot.append(node)\n",
    "\n",
    "        del ego1_nodes\n",
    "        ego1_nodes = ego2_nodes\n",
    "        del ego2_nodes\n",
    "        del ego2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2ed69-e0da-451e-ae77-641f5c00c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a3a56-cd0b-4616-9721-a8f94e655d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_set.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_set, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e9e69-2716-4935-b36b-bee5718435ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/ego_nodes_tot.pkl', 'wb') as file1:\n",
    "    pickle.dump(ego_nodes_tot, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526a4e0-a0a4-4530-b115-8acb725e52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "for ego in train_set:\n",
    "    for rp in ego:\n",
    "        if len(rp) > 2:\n",
    "            no_rel = []\n",
    "            relation_types = []\n",
    "            for k in range(len(rp)-1):\n",
    "                # relation_types.append(graph.edges[rp[k], rp[k+1]]['relation']) # Uncomment this line for WN18RR and YAGO3 graphs\n",
    "                # relation_types.append(graph.edges[rp[k], rp[k+1]]['display_relation']) # Uncomment this line for PrimeKG graph\n",
    "            for i in range(len(rp)-2):\n",
    "                for j in range(i+2, len(rp)):\n",
    "                    if rp[i] != rp[j] and not graph.has_edge(rp[i], rp[j]) and len(set(relation_types[i:j-1])) > 1:\n",
    "                        no_rel.append((rp[i], rp[j]))\n",
    "            for nr in no_rel:\n",
    "                random_position = random.randint(0, len(rp))\n",
    "                rp.insert(random_position, nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106254ab-db88-482a-a199-af1d5676655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/m_n_values.pkl', 'wb') as f:\n",
    "    pickle.dump(m_n_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae32f6-4f61-4f98-a660-a51d6cf70308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "triples = set()\n",
    "\n",
    "def create_message(role, content):\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "for walks in train_set:\n",
    "    for walk in walks:\n",
    "        chat = [create_message(\"system\", \"You are a chatbot that has to predict the relationship between nodes.\")]\n",
    "        \n",
    "        for i, elem in enumerate(walk):\n",
    "            if i == len(walk) - 1 and isinstance(elem, str):\n",
    "                break\n",
    "            \n",
    "            if isinstance(elem, tuple):\n",
    "                # a, b = elem # Uncomment this line for WN18RR and YAGO3 graphs\n",
    "\n",
    "                # Uncomment these two lines for PrimeKG graph\n",
    "                # a = graph.nodes[elem[0]]['node_name']\n",
    "                # b = graph.nodes[elem[1]]['node_name']\n",
    "                \n",
    "                rel = 'no relation'\n",
    "                triples.add((a, rel, b))\n",
    "            else:\n",
    "                for j in range(i + 1, len(walk)):\n",
    "                    if isinstance(walk[j], str):\n",
    "                        # Uncomment these two lines for WN18RR and YAGO3 graphs\n",
    "                        # a, b = walk[i], walk[j]\n",
    "                        # rel = graph[a][b]['relation']\n",
    "                        \n",
    "                        # Uncomment these three lines for PrimeKG graph\n",
    "                        # a = graph.nodes[walk[i]]['node_name']\n",
    "                        # b = graph.nodes[walk[j]]['node_name']\n",
    "                        # rel = graph[walk[i]][walk[j]]['display_relation']\n",
    "                        \n",
    "                        triples.add((a, rel, b))\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "            user_message = create_message(\"user\", f\"Which is the relationship between the node '{a}' and the node '{b}'?\")\n",
    "            assistant_message = create_message(\"assistant\", rel)\n",
    "            chat.extend([user_message, assistant_message])\n",
    "\n",
    "        if len(chat) > 1 and chat not in train_dataset:\n",
    "            train_dataset.append(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f94188-9d02-430b-a74b-51b168c88ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_val_triples.pkl', 'wb') as file1:\n",
    "    pickle.dump(triples, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5be901-a31b-4766-925a-d3c38faf4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea7010-9ccd-497e-be29-73c5d9237082",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = int(len(train_dataset)/10)\n",
    "val_dataset = train_dataset[:l]\n",
    "train_dataset = train_dataset[l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a5e8e-2e30-4f15-9892-6f224365bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ba147-4282-47ff-b26d-19dd262cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'wb') as file2:\n",
    "    pickle.dump(val_dataset, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf9ee7-92f2-4163-b12b-961bc7bbdebe",
   "metadata": {},
   "source": [
    "### Model and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb2116-2435-4a95-b6ce-16eb81bb3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'rb') as file:\n",
    "    train_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9d109-3e3f-4434-980a-f1beb20d516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'rb') as file:\n",
    "    val_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70256cee-2f3f-4aa7-b07e-6af994b9cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_dataset):\n",
    "    if len(t) < 3:\n",
    "        print(t)\n",
    "        del train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55249d26-6730-437b-a44b-197a77f43ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(val_dataset):\n",
    "    if len(t) < 3:\n",
    "        print(t)\n",
    "        del val_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41d426-561e-4d1b-81ff-f7c3f94f9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}\\\n",
    "                            {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}\\\n",
    "                            {{ '<|im_start|>assistant\\n' }}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a395cb-ef80-4e33-a0ba-43e745d5248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset1 = Dataset.from_dict({\"chat\": train_dataset})\n",
    "dataset1 = dataset1.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})\n",
    "\n",
    "dataset2 = Dataset.from_dict({\"chat\": val_dataset})\n",
    "dataset2 = dataset2.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc8346-c18d-4ebe-9272-121ff75c973f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590def80-abf3-4bf5-b8d0-6018a0f75b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    do_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=250,\n",
    "    warmup_steps=50,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset1['formatted_chat'],\n",
    "    eval_dataset=dataset2['formatted_chat'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e2b2-4d78-46aa-9a48-17fc9b90ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('experiments/EXP' + EXP + 'model')\n",
    "tokenizer.save_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ae114-c204-4860-97ed-fb71f82f9211",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403fbc4-16f9-4b08-812c-33853782c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af420a52-0df4-423e-ac5e-0f510679deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('experiments/EXP' + EXP + 'model')\n",
    "final_model = AutoModelForCausalLM.from_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a4f6c-1545-4bbe-9477-291563655f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/test_dataset.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6e7a3-144b-4627-a352-2167b9c6e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ft = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    prompt = []\n",
    "    ground_truth = ''\n",
    "    for m in test_dataset[i]:\n",
    "        if m['role'] != 'assistant':\n",
    "            prompt.append(m)\n",
    "        else:\n",
    "            ground_truth = m['content']\n",
    "    inputs = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v for k, v in inputs.items()}\n",
    "\n",
    "    if ground_truth != '':\n",
    "        tok = tokenizer(ground_truth, return_tensors=\"pt\")\n",
    "        out = final_model.generate(**inputs, max_new_tokens=len(tok['input_ids'][0]), do_sample=True, num_return_sequences=10)\n",
    "        generations = []\n",
    "        for j in range(10):\n",
    "            gen = tokenizer.decode(out[j][len(inputs[\"input_ids\"][0]):])\n",
    "            generations.append(gen)\n",
    "    \n",
    "        t = (test_dataset[i], generations)\n",
    "        post_ft.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65462-ed91-45d8-872b-061fbfe0d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/results_postft.pkl', 'wb') as file:\n",
    "    pickle.dump(post_ft, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14f42a-1535-4daf-a098-223682317a35",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b14a38-6a79-43e3-8bf4-0db36781d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('experiments/EXP' + EXP + '/results_postft.pkl', 'rb') as file:\n",
    "    post_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60adff-c798-46f2-88cc-8f98368cbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for d, g in post_ft:\n",
    "    gt = d[2]['content'].strip()\n",
    "    gen = []\n",
    "    for pred in g:\n",
    "        p = pred.strip()\n",
    "        if len(p) > len(gt):\n",
    "            p = p[:len(gt)]\n",
    "        gen.append(p)\n",
    "    clean.append((d, gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b639b-cf8e-49e1-87ca-da9372d3b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    try:\n",
    "        mrr = g.index(c) + 1\n",
    "    except ValueError:\n",
    "        mrr = 10\n",
    "    mrr_list.append(1/mrr)\n",
    "\n",
    "mrr_mean = sum(mrr_list) / len(mrr_list)\n",
    "mrr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdd379-ab69-464f-be30-899179569315",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit1_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c == g[0]:\n",
    "        hit1_list.append(1)\n",
    "    else:\n",
    "        hit1_list.append(0)\n",
    "\n",
    "hit1_mean = sum(hit1_list) / len(hit1_list)\n",
    "\n",
    "hit3_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:3]:\n",
    "        hit3_list.append(1)\n",
    "    else:\n",
    "        hit3_list.append(0)\n",
    "\n",
    "hit3_mean = sum(hit3_list) / len(hit3_list)\n",
    "\n",
    "hit5_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:5]:\n",
    "        hit5_list.append(1)\n",
    "    else:\n",
    "        hit5_list.append(0)\n",
    "\n",
    "hit5_mean = sum(hit5_list) / len(hit5_list)\n",
    "\n",
    "hit7_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:7]:\n",
    "        hit7_list.append(1)\n",
    "    else:\n",
    "        hit7_list.append(0)\n",
    "\n",
    "hit7_mean = sum(hit7_list) / len(hit7_list)\n",
    "\n",
    "hit10_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g:\n",
    "        hit10_list.append(1)\n",
    "    else:\n",
    "        hit10_list.append(0)\n",
    "\n",
    "hit10_mean = sum(hit10_list) / len(hit10_list)\n",
    "\n",
    "print(\"Hit@1 = \" + str(hit1_mean))\n",
    "print(\"Hit@3 = \" + str(hit3_mean))\n",
    "print(\"Hit@5 = \" + str(hit5_mean))\n",
    "print(\"Hit@7 = \" + str(hit7_mean))\n",
    "print(\"Hit@10 = \" + str(hit10_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc850b-108d-435b-8e5c-5483e283fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "\n",
    "# Uncomment for WN18RR graph\n",
    "# class_labels = [\n",
    "#     'derivationally related form', 'has part', 'hyponym', 'hypernym', 'instance hypernym',\n",
    "#     'instance hyponym', 'member holonym', 'member meronym', 'member of domain topic',\n",
    "#     'no relation', 'part of', 'synset domain topic of'\n",
    "# ]\n",
    "\n",
    "# Uncomment for YAGO3 graph\n",
    "# class_labels = [\n",
    "#     'actedIn', 'created', 'diedIn', 'directed', 'graduatedFrom', 'happenedIn', 'hasChild',\n",
    "#     'hasGender', 'hasMusicalRole', 'hasWonPrize', 'influences', 'isAffiliatedTo', 'isConnectedTo',\n",
    "#     'isLocatedIn', 'no relation', 'participatedIn', 'playsFor', 'wasBornIn', 'wroteMusicFor'\n",
    "# ]\n",
    "\n",
    "# Uncomment for PrimeKG graph\n",
    "# class_labels = [\n",
    "#     'associated with', 'contraindication', 'enzyme', 'expression absent', 'expression present',\n",
    "#     'indication', 'interacts with', 'no relation', 'off-label use', 'parent-child',\n",
    "#     'phenotype absent', 'phenotype present', 'ppi', 'side effect',\n",
    "#     'synergistic interaction', 'target', 'transporter'\n",
    "# ]\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for d, g in clean:\n",
    "    true_label = d[2]['content']\n",
    "    prediction_text = g[0]\n",
    "\n",
    "    predicted_label = None\n",
    "    for label in class_labels:\n",
    "        if re.search(r'\\b' + re.escape(label) + r'\\b', prediction_text):\n",
    "            predicted_label = label\n",
    "            break\n",
    "\n",
    "    predicted_label = predicted_label if predicted_label else \"unknown\"\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "filtered_y_true = []\n",
    "filtered_y_pred = []\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    if p != \"unknown\":\n",
    "        filtered_y_true.append(t)\n",
    "        filtered_y_pred.append(p)\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    filtered_y_true, filtered_y_pred, labels=class_labels, average=None\n",
    ")\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Classe: {label}\")\n",
    "    print(f\"  Precision: {precision[i]:.2f}\")\n",
    "    print(f\"  Recall:    {recall[i]:.2f}\")\n",
    "    print(f\"  F1-score:  {f1[i]:.2f}\")\n",
    "    print(f\"  Support:   {support[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b5943-ab9b-4128-bd4e-fba24c46088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    filtered_y_true, filtered_y_pred, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Weighted Precision: {precision_weighted:.2f}\")\n",
    "print(f\"Weighted Recall:    {recall_weighted:.2f}\")\n",
    "print(f\"Weighted F1-score:  {f1_weighted:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
