{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251590e0-6fb8-483a-bd90-d38b6660220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd1630-ef7d-40c4-85df-b3ea77dad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1ffb-2ad8-40a5-8619-435f53de64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "newpath = 'experiments/EXP' + EXP\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "if not os.path.exists(newpath + 'model'):\n",
    "    os.makedirs(newpath + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29459037-56ce-4448-b03d-096fbfab1cbb",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac92d7-2e3e-48d8-8638-9474b5ed0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph = nx.read_graphml(\"GRAPH_PATH.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5456ee-38de-4af0-9164-62f1182a1ffd",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc214b9-83b8-4380-a48b-b7e9eaa11757",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/edges_to_remove.pkl', 'rb') as file:\n",
    "    edges_to_remove = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaeed1c-04fc-4269-a4ef-d38660fa0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for e in edges_to_remove:\n",
    "    if graph.has_edge(e[0], e[1]):\n",
    "        graph.remove_edge(e[0], e[1])\n",
    "        cnt += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6793fa-36f7-49ce-8ca5-d0421e77636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/test_dataset.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7df00-3430-4f11-9d05-bf4cf712ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/train_dataset.pkl', 'rb') as file:\n",
    "    train_dataset_cofine = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a67188-c4ca-4252-a64f-041da0b2ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/val_dataset.pkl', 'rb') as file:\n",
    "    val_dataset_cofine = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fafd4-091e-4fcd-a1c0-ce591d63ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chat = len(train_dataset_cofine) + len(val_dataset_cofine)\n",
    "num_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568ce40-37cc-4a5e-9df4-1cc07e446a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset_cofine\n",
    "del val_dataset_cofine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e0fec-7dd9-4826-9006-39f47e3d5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644495a-bfd2-4a41-abc4-a3fa06be94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2621f9-0c08-4ee6-97a8-d40a98796bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.choices(nodes, k=num_chat)\n",
    "del nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a5275-8783-4426-846a-e653bb3121e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "train_set = []\n",
    "path_length = 4\n",
    "\n",
    "for _ in range(num_chat):\n",
    "    eds = random.choices(edges, k=path_length)\n",
    "    path = []\n",
    "    for e in eds:\n",
    "        path.append(e)\n",
    "\n",
    "    train_set.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adacdb-ec15-4bff-a16d-7d578e103577",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_set.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_set, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65635b-6f11-4d6e-871f-d8ddde2ffbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_set.pkl', 'rb') as file:\n",
    "    train_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526a4e0-a0a4-4530-b115-8acb725e52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "cnt = 0\n",
    "cnt2 = 0\n",
    "\n",
    "for rp in train_set:\n",
    "    if len(rp) > 2:\n",
    "        added = False\n",
    "        iterations = random.randint(0,3)\n",
    "        for _ in range(iterations):\n",
    "            tpl1 = random.randint(0, len(rp)-1)\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                tpl2 = random.randint(0, len(rp)-1)\n",
    "                if tpl1 != tpl2:\n",
    "                    ok = True\n",
    "            elem1 = random.randint(0, 1)\n",
    "            elem2 = random.randint(0, 1)\n",
    "            if not graph.has_edge(rp[tpl1][elem1], rp[tpl2][elem2]) and (rp[tpl1][elem1], rp[tpl2][elem2]) not in rp:\n",
    "                i = random.randint(0, len(rp)-1)\n",
    "                rp.insert(i, (rp[tpl1][elem1], rp[tpl2][elem2]))\n",
    "                cnt += 1\n",
    "                added = True\n",
    "        if added:\n",
    "            cnt2 += 1\n",
    "\n",
    "print(\"'no relation' added: \" + str(cnt))\n",
    "print(\"Number of random path modified: \" + str(cnt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3a476-cbea-443a-958c-0a75ccfc0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "triples = set()\n",
    "\n",
    "for walk in train_set:\n",
    "    chat = []\n",
    "    system = {\"role\": \"system\", \"content\": \"You are a chatbot that has to predict the relationship between nodes.\"}\n",
    "    chat.append(system)\n",
    "    \n",
    "    for wk in walk:\n",
    "        if graph.has_edge(wk[0], wk[1]):\n",
    "            # Uncomment these lines for WN18RR and YAGO3 graphs\n",
    "            # rel = graph[wk[0]][wk[1]]['relation']\n",
    "            # a = wk[0]\n",
    "            # b = wk[1]\n",
    "\n",
    "            # Uncomment these lines for PrimeKG graph\n",
    "            # rel = graph[wk[0]][wk[1]]['display_relation']\n",
    "            # a = graph.nodes[wk[0]]['node_name']\n",
    "            # b = graph.nodes[wk[1]]['node_name']\n",
    "    \n",
    "            triple = (a, rel, b)\n",
    "            triples.add(triple)\n",
    "            \n",
    "            user = \"Which is the relationship between the node '\" + a + \"' and the node '\" + b + \"'?\"\n",
    "            assistant = rel\n",
    "            \n",
    "            message = {}\n",
    "            message[\"role\"] = \"user\"\n",
    "            message[\"content\"] = user\n",
    "            chat.append(message)\n",
    "    \n",
    "            message = {}\n",
    "            message[\"role\"] = \"assistant\"\n",
    "            message[\"content\"] = assistant\n",
    "            chat.append(message)\n",
    "        else:\n",
    "            rel = 'no relation'\n",
    "\n",
    "            # Uncomment these lines for WN18RR and YAGO3 graphs\n",
    "            # a = wk[0]\n",
    "            # b = wk[1]\n",
    "\n",
    "            # Uncomment these lines for PrimeKG graph\n",
    "            # a = graph.nodes[wk[0]]['node_name']\n",
    "            # b = graph.nodes[wk[1]]['node_name']\n",
    "\n",
    "            triple = (a, rel, b)\n",
    "            triples.add(triple)\n",
    "            \n",
    "            user = \"Which is the relationship between the node '\" + a + \"' and the node '\" + b + \"'?\"\n",
    "            assistant = rel\n",
    "            \n",
    "            message = {}\n",
    "            message[\"role\"] = \"user\"\n",
    "            message[\"content\"] = user\n",
    "            chat.append(message)\n",
    "\n",
    "            message = {}\n",
    "            message[\"role\"] = \"assistant\"\n",
    "            message[\"content\"] = assistant\n",
    "            chat.append(message)\n",
    "\n",
    "    if chat not in train_dataset:\n",
    "        train_dataset.append(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f94188-9d02-430b-a74b-51b168c88ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_val_triples.pkl', 'wb') as file1:\n",
    "    pickle.dump(triples, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5be901-a31b-4766-925a-d3c38faf4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea7010-9ccd-497e-be29-73c5d9237082",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = int(len(train_dataset)/10)\n",
    "val_dataset = train_dataset[:l]\n",
    "train_dataset = train_dataset[l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a5e8e-2e30-4f15-9892-6f224365bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ba147-4282-47ff-b26d-19dd262cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'wb') as file2:\n",
    "    pickle.dump(val_dataset, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf9ee7-92f2-4163-b12b-961bc7bbdebe",
   "metadata": {},
   "source": [
    "### Acquisizione modello e dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb2116-2435-4a95-b6ce-16eb81bb3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'rb') as file:\n",
    "    train_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9d109-3e3f-4434-980a-f1beb20d516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'rb') as file:\n",
    "    val_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36671b-ca83-4f47-b3f4-f3a2c1ad482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_dataset):\n",
    "    if len(t) < 3:\n",
    "        print(t)\n",
    "        del train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab488f7-98a5-4cb0-8a8e-fce3af1d348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(val_dataset):\n",
    "    if len(t) < 3:\n",
    "        print(t)\n",
    "        del val_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b69153-e0c7-40fa-adf7-62027ca6cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68466-ea8c-4692-93f2-b8fd00add250",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(val_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41d426-561e-4d1b-81ff-f7c3f94f9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}\\\n",
    "                            {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}\\\n",
    "                            {{ '<|im_start|>assistant\\n' }}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a395cb-ef80-4e33-a0ba-43e745d5248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset1 = Dataset.from_dict({\"chat\": train_dataset})\n",
    "dataset1 = dataset1.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})\n",
    "\n",
    "dataset2 = Dataset.from_dict({\"chat\": val_dataset})\n",
    "dataset2 = dataset2.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc8346-c18d-4ebe-9272-121ff75c973f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590def80-abf3-4bf5-b8d0-6018a0f75b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    do_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=250,\n",
    "    warmup_steps=50,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset1['formatted_chat'],\n",
    "    eval_dataset=dataset2['formatted_chat'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e2b2-4d78-46aa-9a48-17fc9b90ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('experiments/EXP' + EXP + 'model')\n",
    "tokenizer.save_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ae114-c204-4860-97ed-fb71f82f9211",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403fbc4-16f9-4b08-812c-33853782c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af420a52-0df4-423e-ac5e-0f510679deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('experiments/EXP' + EXP + 'model')\n",
    "final_model = AutoModelForCausalLM.from_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a4f6c-1545-4bbe-9477-291563655f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/test_dataset.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6e7a3-144b-4627-a352-2167b9c6e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ft = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    prompt = []\n",
    "    ground_truth = ''\n",
    "    for m in test_dataset[i]:\n",
    "        if m['role'] != 'assistant':\n",
    "            prompt.append(m)\n",
    "        else:\n",
    "            ground_truth = m['content']\n",
    "    inputs = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v for k, v in inputs.items()}\n",
    "\n",
    "    if ground_truth != '':\n",
    "        tok = tokenizer(ground_truth, return_tensors=\"pt\")\n",
    "        out = final_model.generate(**inputs, max_new_tokens=len(tok['input_ids'][0]), do_sample=True, num_return_sequences=10)\n",
    "        generations = []\n",
    "        for j in range(10):\n",
    "            gen = tokenizer.decode(out[j][len(inputs[\"input_ids\"][0]):])\n",
    "            generations.append(gen)\n",
    "    \n",
    "        t = (test_dataset[i], generations)\n",
    "        post_ft.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65462-ed91-45d8-872b-061fbfe0d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/results_postft.pkl', 'wb') as file:\n",
    "    pickle.dump(post_ft, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14f42a-1535-4daf-a098-223682317a35",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b14a38-6a79-43e3-8bf4-0db36781d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('experiments/EXP' + EXP + '/results_postft.pkl', 'rb') as file:\n",
    "    post_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0de675-e40e-4203-a5ae-acb9b0115927",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for d, g in post_ft:\n",
    "    gt = d[2]['content'].strip()\n",
    "    gen = []\n",
    "    for pred in g:\n",
    "        p = pred.strip()\n",
    "        if len(p) > len(gt):\n",
    "            p = p[:len(gt)]\n",
    "        gen.append(p)\n",
    "    clean.append((d, gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b639b-cf8e-49e1-87ca-da9372d3b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    try:\n",
    "        mrr = g.index(c) + 1\n",
    "    except ValueError:\n",
    "        mrr = 10\n",
    "    mrr_list.append(1/mrr)\n",
    "\n",
    "mrr_mean = sum(mrr_list) / len(mrr_list)\n",
    "mrr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdd379-ab69-464f-be30-899179569315",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit1_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c == g[0]:\n",
    "        hit1_list.append(1)\n",
    "    else:\n",
    "        hit1_list.append(0)\n",
    "\n",
    "hit1_mean = sum(hit1_list) / len(hit1_list)\n",
    "\n",
    "hit3_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:3]:\n",
    "        hit3_list.append(1)\n",
    "    else:\n",
    "        hit3_list.append(0)\n",
    "\n",
    "hit3_mean = sum(hit3_list) / len(hit3_list)\n",
    "\n",
    "hit5_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:5]:\n",
    "        hit5_list.append(1)\n",
    "    else:\n",
    "        hit5_list.append(0)\n",
    "\n",
    "hit5_mean = sum(hit5_list) / len(hit5_list)\n",
    "\n",
    "hit7_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g[:7]:\n",
    "        hit7_list.append(1)\n",
    "    else:\n",
    "        hit7_list.append(0)\n",
    "\n",
    "hit7_mean = sum(hit7_list) / len(hit7_list)\n",
    "\n",
    "hit10_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content'].strip()\n",
    "    if c in g:\n",
    "        hit10_list.append(1)\n",
    "    else:\n",
    "        hit10_list.append(0)\n",
    "\n",
    "hit10_mean = sum(hit10_list) / len(hit10_list)\n",
    "\n",
    "print(\"Hit@1 = \" + str(hit1_mean))\n",
    "print(\"Hit@3 = \" + str(hit3_mean))\n",
    "print(\"Hit@5 = \" + str(hit5_mean))\n",
    "print(\"Hit@7 = \" + str(hit7_mean))\n",
    "print(\"Hit@10 = \" + str(hit10_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee5d0d-c98a-47c9-8927-ff8cd7a473f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "\n",
    "# Uncomment for WN18RR graph\n",
    "# class_labels = [\n",
    "#     'derivationally related form', 'has part', 'hyponym', 'hypernym', 'instance hypernym',\n",
    "#     'instance hyponym', 'member holonym', 'member meronym', 'member of domain topic',\n",
    "#     'no relation', 'part of', 'synset domain topic of'\n",
    "# ]\n",
    "\n",
    "# Uncomment for YAGO3 graph\n",
    "# class_labels = [\n",
    "#     'actedIn', 'created', 'diedIn', 'directed', 'graduatedFrom', 'happenedIn', 'hasChild',\n",
    "#     'hasGender', 'hasMusicalRole', 'hasWonPrize', 'influences', 'isAffiliatedTo', 'isConnectedTo',\n",
    "#     'isLocatedIn', 'no relation', 'participatedIn', 'playsFor', 'wasBornIn', 'wroteMusicFor'\n",
    "# ]\n",
    "\n",
    "# Uncomment for PrimeKG graph\n",
    "# class_labels = [\n",
    "#     'associated with', 'contraindication', 'enzyme', 'expression absent', 'expression present',\n",
    "#     'indication', 'interacts with', 'no relation', 'off-label use', 'parent-child',\n",
    "#     'phenotype absent', 'phenotype present', 'ppi', 'side effect',\n",
    "#     'synergistic interaction', 'target', 'transporter'\n",
    "# ]\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for d, g in clean:\n",
    "    true_label = d[2]['content']\n",
    "    prediction_text = g[0]\n",
    "\n",
    "    predicted_label = None\n",
    "    for label in class_labels:\n",
    "        if re.search(r'\\b' + re.escape(label) + r'\\b', prediction_text):\n",
    "            predicted_label = label\n",
    "            break\n",
    "\n",
    "    predicted_label = predicted_label if predicted_label else \"unknown\"\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "filtered_y_true = []\n",
    "filtered_y_pred = []\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    if p != \"unknown\":\n",
    "        filtered_y_true.append(t)\n",
    "        filtered_y_pred.append(p)\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    filtered_y_true, filtered_y_pred, labels=class_labels, average=None\n",
    ")\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Classe: {label}\")\n",
    "    print(f\"  Precision: {precision[i]:.2f}\")\n",
    "    print(f\"  Recall:    {recall[i]:.2f}\")\n",
    "    print(f\"  F1-score:  {f1[i]:.2f}\")\n",
    "    print(f\"  Support:   {support[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffa9ee-2121-4c0e-ab63-f2b017ed888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    filtered_y_true, filtered_y_pred, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Weighted Precision: {precision_weighted:.2f}\")\n",
    "print(f\"Weighted Recall:    {recall_weighted:.2f}\")\n",
    "print(f\"Weighted F1-score:  {f1_weighted:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
